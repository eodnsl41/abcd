{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi1FGmt2mdvUzKYz1azR3y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eodnsl41/abcd/blob/main/Neural_Net_%26_back_propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Affine layer : Hidden layer끼리 연결\n",
        "\n",
        "import numpy as np\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W #변수 설정. weight, bias\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        #가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b #x가 들어오면 dot product 연산을 함\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout): #역전파. \n",
        "        dx  = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis = 0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "b46EbY4yKLZS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#softmax는 값을 정규화 시킨다.\n",
        "#전체의 합을 1로 만드는게 softmax의 주된 목적\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2: #ndim : 앞에 있는 matrix의 차원확인\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
        "        return y.T\n",
        "    x = x - np.max(x) #너무 큰 값이 나오지 않기 위해 max값을 뺌\n",
        "\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n"
      ],
      "metadata": {
        "id": "SMkrbubWNX75"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#많은 activation functions 중 ReLU 함수 정의하는 방법\n",
        "\n",
        "# 순전파때는 0 이상일때만 인풋 그대로 아웃풋을 해주는게 렐루함수\n",
        "#0 이하일 때는 그냥 값을 0으로 해주는게 순전파 때 역할\n",
        "# 역전파때는 그냥 그대로 넣어주면 됨\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "NkDZ8iqyO6yK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.matrixlib.defmatrix import N\n",
        "#one-hot vector\n",
        "\n",
        "def cross_entropy_error(y,t): \n",
        "    #y값 : 신경망 예측한 y 프레딕션 값\n",
        "    #t값 : 우리가 갖고 있는 라벨링 데이터 값\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        #에러 방지 조건\n",
        "\n",
        "    #훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t= t.argmax(axis =1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return - np.sum(np.log(y[np.arrange(batch_size), t] + 1e-7)) / batch_size\n",
        "    #10의 -7승 더해준 이유 : log0이 존재하면 에러가 걸리니까 작은값이라도 더해주는거\n",
        "\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None #손실함수\n",
        "        self.y = None #softmax의 출력\n",
        "        self.t = None #정답레이블 (원-핫 인코딩의 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        #비교를 해야하니까 라벨데이터 갖고옴\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1): \n",
        "        #역전파때에는 이거에 미분을 한 거를 비교를 해서 전달을 하게 됨\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size:\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arrange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "hakCdv6LQAG5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#실제 미분값 구해서 학습 시키는것 / 역전파때 함수 구하는 것 -> 비교해봄\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h= 1e-6 #0.000001 #굉장히 작은 값\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    it = np.nditer(x, flags = ['multi_index'], op_flags =['readwrite']) #np.nditer는 편미분. 인풋값에 따라서 편미분\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) #f(x+h)\n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x) #f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val #값 복원\n",
        "        it.iternext()\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "-s19VAv_TjOA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feedforward network 정의\n",
        "\n",
        "from collections import OrderedDict #딕셔너리는 순서가 없는데 순서를 만든거임\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        #가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1']= weight_init_std * np.random.randn(input_size, hidden_size) #weight의 초기값을 랜덤하게 정의한 것\n",
        "        self.params['b1']= np.zeros(hidden_size) \n",
        "        self.params['W2']= weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2']= np.zeros(output_size)\n",
        "\n",
        "        #계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) #affine은 히든레이어\n",
        "        self.layers['Relu1'] = Relu() #relu function 하나 넣음\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x): #신경망 예측치를 만드는게 prediction 함수\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력데이터, t:정답레이블\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y,t) #순전파 사용\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis = 1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis =1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W : self.loss(x,t)\n",
        "\n",
        "        grads ={}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        #forward\n",
        "        self.loss(x,t)\n",
        "\n",
        "        #backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        #결과 저장\n",
        "        grads ={}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ],
      "metadata": {
        "id": "gdi8jYtpVvD2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mnist\n",
        "#~~"
      ],
      "metadata": {
        "id": "u96QvPeJYnvL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}